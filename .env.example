# ============================================================================
# Hephaestus LLM Provider Configuration
# ============================================================================
# Copy this file to .env and configure your preferred LLM providers
#
# PROVIDER OPTIONS:
# - Cloud APIs: OpenAI, Anthropic, Groq, OpenRouter
# - Local: LM Studio (runs on your machine)
# - Subscriptions: Anthropic subscription, Zai subscription
#
# EMBEDDING OPTIONS:
# - OpenAI API: text-embedding-3-large (cloud, paid per token)
# - LM Studio: Local embedding server (free, runs on your machine)
# ============================================================================

# ============================================================================
# EMBEDDINGS CONFIGURATION (Required for vector search)
# ============================================================================

# Option 1: OpenAI Embeddings (Cloud, Recommended)
# - Model: text-embedding-3-large
# - Pros: High quality, fast, reliable
# - Cons: Costs per token, requires internet
# - Use when: You want best quality and have API budget
EMBEDDING_PROVIDER=openai
OPENAI_API_KEY=sk-proj-your-key-here

# Option 2: LM Studio Embeddings (Local, Free)
# - Model: Any embedding model loaded in LM Studio
# - Pros: Free, private, no internet needed
# - Cons: Slower, requires local GPU/CPU, setup required
# - Use when: You want privacy or have no API budget
# EMBEDDING_PROVIDER=lmstudio
# LMSTUDIO_EMBEDDING_URL=http://localhost:1234/v1
# LMSTUDIO_EMBEDDING_MODEL=text-embedding-ada-002  # Or any model you loaded

# ============================================================================
# LLM INFERENCE CONFIGURATION (For text generation)
# ============================================================================

# ----------------------------------------------------------------------------
# OPTION 1: OpenAI (Cloud API)
# ----------------------------------------------------------------------------
# - Models: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# - Pros: High quality, fast, reliable
# - Cons: Costs per token, requires internet
USE_OPENAI=true
# OPENAI_API_KEY is already set above for embeddings
OPENAI_MODEL=gpt-4-turbo
OPENAI_BASE_URL=https://api.openai.com/v1  # Default, usually no need to change

# ----------------------------------------------------------------------------
# OPTION 2: Anthropic (Cloud API or Subscription)
# ----------------------------------------------------------------------------
# - Models: claude-3-opus, claude-3-sonnet, claude-3-haiku
# - Pros: Excellent reasoning, large context windows
# - Cons: Costs per token (if using API)
#
# ANTHROPIC HAS THREE ACCESS MODES:
#
# Mode A: Anthropic API (Pay-per-token)
# - You have an Anthropic API key
# - Billed based on token usage
USE_ANTHROPIC_API=true
ANTHROPIC_API_KEY=sk-ant-your-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229
ANTHROPIC_BASE_URL=https://api.anthropic.com  # Default

# Mode B: Anthropic Subscription (claude.ai subscription)
# - You have a Claude Pro or Team subscription at claude.ai
# - Uses your subscription quota, not billed per token
# - May require session cookies or organization setup
# USE_ANTHROPIC_API=false
# ANTHROPIC_API_KEY=  # Leave empty if using subscription
# ANTHROPIC_SESSION_COOKIE=your-session-cookie-here  # From claude.ai browser

# Mode C: Zai Subscription (Organization-level access)
# - Your organization has Zai subscription with Anthropic access
# - Uses organization credentials
# - May not require individual API key
# USE_ANTHROPIC_API=false
# ZAI_SUBSCRIPTION=true
# ZAI_ORG_ID=your-org-id-here
# ANTHROPIC_API_KEY=  # May be provided by Zai, or leave empty

# ----------------------------------------------------------------------------
# OPTION 3: Groq (Cloud API - Fast inference)
# ----------------------------------------------------------------------------
# - Models: llama2-70b, mixtral-8x7b, gemma-7b
# - Pros: Very fast inference, free tier available
# - Cons: Limited model selection, newer service
GROQ_ENABLED=true
GROQ_API_KEY=gsk_your-key-here
GROQ_MODEL=mixtral-8x7b-32768
GROQ_BASE_URL=https://api.groq.com/openai/v1  # Default

# ----------------------------------------------------------------------------
# OPTION 4: OpenRouter (Multi-provider aggregator)
# ----------------------------------------------------------------------------
# - Models: Access to OpenAI, Anthropic, Meta, and many others
# - Pros: One API for many models, competitive pricing
# - Cons: Adds latency, requires account
OPENROUTER_ENABLED=true
OPENROUTER_API_KEY=sk-or-v1-your-key-here
OPENROUTER_MODEL=anthropic/claude-3-sonnet
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1  # Default

# ----------------------------------------------------------------------------
# OPTION 5: LM Studio (Local inference - FREE)
# ----------------------------------------------------------------------------
# - Models: Any GGUF model you download (Llama, Mistral, etc.)
# - Pros: FREE, private, no internet needed, full control
# - Cons: Requires local GPU/CPU, slower than cloud, setup required
# - Use when: Privacy is critical, no API budget, or offline work
#
# SETUP INSTRUCTIONS:
# 1. Download LM Studio from https://lmstudio.ai/
# 2. Download a model (e.g., Llama-2-7B, Mistral-7B, CodeLlama)
# 3. Start the local server in LM Studio (usually port 1234)
# 4. Enable LM Studio below and disable other providers
#
# RECOMMENDED MODELS FOR LM STUDIO:
# - Code: TheBloke/CodeLlama-13B-GGUF
# - General: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
# - Reasoning: TheBloke/Llama-2-13B-chat-GGUF
#
LMSTUDIO_ENABLED=true
LMSTUDIO_BASE_URL=http://localhost:1234/v1  # Default LM Studio server
LMSTUDIO_MODEL=local-model  # Use "local-model" or the exact model name loaded
# LMSTUDIO_API_KEY=  # Usually not needed for local server

# ============================================================================
# PROVIDER PRIORITY AND FALLBACK
# ============================================================================
# If multiple providers are enabled, Hephaestus will try them in this order:
# 1. LM Studio (if enabled and running)
# 2. Anthropic (if enabled)
# 3. OpenAI (if enabled)
# 4. Groq (if enabled)
# 5. OpenRouter (if enabled)
#
# To use ONLY LM Studio (local, free):
# - Set LMSTUDIO_ENABLED=true
# - Set USE_OPENAI=false
# - Set USE_ANTHROPIC_API=false
# - Set GROQ_ENABLED=false
# - Set OPENROUTER_ENABLED=false
# - Set EMBEDDING_PROVIDER=lmstudio

# ============================================================================
# TESTING CONFIGURATIONS
# ============================================================================

# Configuration 1: All Local (LM Studio for everything)
# EMBEDDING_PROVIDER=lmstudio
# LMSTUDIO_EMBEDDING_URL=http://localhost:1234/v1
# LMSTUDIO_ENABLED=true
# LMSTUDIO_BASE_URL=http://localhost:1234/v1
# USE_OPENAI=false
# USE_ANTHROPIC_API=false
# GROQ_ENABLED=false
# OPENROUTER_ENABLED=false

# Configuration 2: OpenAI embeddings + LM Studio inference
# EMBEDDING_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-your-key-here
# LMSTUDIO_ENABLED=true
# LMSTUDIO_BASE_URL=http://localhost:1234/v1
# USE_OPENAI=false  # Don't use OpenAI for inference
# USE_ANTHROPIC_API=false
# GROQ_ENABLED=false
# OPENROUTER_ENABLED=false

# Configuration 3: OpenAI embeddings + Anthropic subscription
# EMBEDDING_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-your-key-here
# USE_ANTHROPIC_API=false  # Using subscription, not API
# ANTHROPIC_SESSION_COOKIE=your-session-cookie-here
# USE_OPENAI=false
# LMSTUDIO_ENABLED=false
# GROQ_ENABLED=false
# OPENROUTER_ENABLED=false

# Configuration 4: OpenAI embeddings + Groq inference (fast and cheap)
# EMBEDDING_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-your-key-here
# GROQ_ENABLED=true
# GROQ_API_KEY=gsk_your-key-here
# USE_OPENAI=false
# USE_ANTHROPIC_API=false
# LMSTUDIO_ENABLED=false
# OPENROUTER_ENABLED=false

# ============================================================================
# ADVANCED CONFIGURATION
# ============================================================================

# Custom endpoints (if using proxies or custom deployments)
# OPENAI_BASE_URL=https://your-proxy.com/v1
# ANTHROPIC_BASE_URL=https://your-proxy.com
# GROQ_BASE_URL=https://your-proxy.com/v1
# LMSTUDIO_BASE_URL=http://192.168.1.100:1234/v1  # Remote LM Studio

# Timeout settings (in seconds)
# LLM_TIMEOUT=120
# EMBEDDING_TIMEOUT=60

# Retry settings
# LLM_MAX_RETRIES=3
# LLM_RETRY_DELAY=1

# Context window sizes (adjust based on your model)
# OPENAI_MAX_TOKENS=4096
# ANTHROPIC_MAX_TOKENS=100000
# GROQ_MAX_TOKENS=32768
# LMSTUDIO_MAX_TOKENS=4096

# ============================================================================
# DATABASE AND VECTOR STORE
# ============================================================================

# Qdrant Configuration (Vector database for embeddings)
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION_PREFIX=hephaestus

# SQLite Database (Task and workflow storage)
DATABASE_PATH=./hephaestus.db

# ============================================================================
# MCP SERVER CONFIGURATION
# ============================================================================

MCP_PORT=8000
MCP_HOST=0.0.0.0

# ============================================================================
# MONITORING AND PERFORMANCE
# ============================================================================

MONITORING_INTERVAL_SECONDS=60
MAX_HEALTH_CHECK_FAILURES=3
AGENT_TIMEOUT_MINUTES=30
MAX_CONCURRENT_AGENTS=10

# Default CLI Tool
DEFAULT_CLI_TOOL=claude  # or "codex"
