# ============================================================================
# Hephaestus Configuration
# ============================================================================
# Copy this file to .env and configure your providers
#
# THREE PARTS TO CONFIGURE:
# 1. CLAUDE CODE LLM PROVIDER - For main inference (Anthropic subscription, Zai, or API key)
# 2. EMBEDDING PROVIDER - For vector search (OpenAI or LM Studio)
# 3. CODE REVIEW PROVIDER - For code review only (Groq or LM Studio)
# ============================================================================

# ============================================================================
# PART 1: CLAUDE CODE LLM PROVIDER (Main Inference)
# ============================================================================
# This is the primary LLM used by Claude Code for agent execution and tasks
#
# THREE OPTIONS:
# Option A: Anthropic Subscription (claude.ai subscription)
# Option B: Zai Subscription (organization-level access)
# Option C: Anthropic API Key (pay-per-token)

# Option A: Anthropic Subscription (Claude Pro or Team at claude.ai)
# - You have a Claude Pro or Team subscription
# - Uses subscription quota, not billed per token
# - May require session cookies or organization setup
CLAUDE_CODE_PROVIDER=anthropic-subscription
ANTHROPIC_SUBSCRIPTION=true
# ANTHROPIC_SESSION_COOKIE=your-session-cookie-here  # From claude.ai browser if needed
# ANTHROPIC_API_KEY=  # Leave empty when using subscription

# Option B: Zai Subscription (Organization-level Anthropic access)
# - Your organization has Zai subscription with Anthropic access
# - Uses organization credentials
# CLAUDE_CODE_PROVIDER=zai-subscription
# ZAI_SUBSCRIPTION=true
# ZAI_ORG_ID=your-org-id-here
# ANTHROPIC_API_KEY=  # May be provided by Zai, or leave empty

# Option C: Anthropic API Key (Pay-per-token)
# - You have an Anthropic API key
# - Billed based on token usage
# CLAUDE_CODE_PROVIDER=anthropic-api
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# Anthropic Model Configuration (applies to all options)
ANTHROPIC_MODEL=claude-3-sonnet-20240229
ANTHROPIC_BASE_URL=https://api.anthropic.com  # Default

# ============================================================================
# PART 2: EMBEDDING PROVIDER (Vector Search)
# ============================================================================
# Used for semantic search and vector database operations
#
# TWO OPTIONS:
# Option A: OpenAI Embeddings (Cloud, recommended)
# Option B: LM Studio Embeddings (Local, free)

# Option A: OpenAI Embeddings (Recommended)
# - Model: text-embedding-3-large
# - Pros: High quality, fast, reliable
# - Cons: Costs per token, requires internet
EMBEDDING_PROVIDER=openai
OPENAI_API_KEY=sk-proj-your-key-here
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Option B: LM Studio Embeddings (Local, Free)
# - Default model: nomic-embed-text-v1.5 (our standard choice)
# - Pros: Free, private, no internet needed
# - Cons: Slower, requires local GPU/CPU, setup required
# EMBEDDING_PROVIDER=lmstudio
# LMSTUDIO_EMBEDDING_URL=http://localhost:1234/v1
# LMSTUDIO_EMBEDDING_MODEL=nomic-embed-text-v1.5  # Fixed default

# ============================================================================
# PART 3: CODE REVIEW PROVIDER (Code Review Only)
# ============================================================================
# Used ONLY for code review tasks, not for main inference
#
# TWO OPTIONS:
# Option A: Groq (Cloud API - fast code review)
# Option B: LM Studio (Local models - gptoss120 or gpt20)

# Option A: Groq (Recommended for code review)
# - Models: llama2-70b, mixtral-8x7b, gemma-7b
# - Pros: Very fast inference, good for code review
# - Use case: ONLY for code review, not general inference
CODE_REVIEW_PROVIDER=groq
GROQ_API_KEY=gsk_your-key-here
GROQ_MODEL=mixtral-8x7b-32768
GROQ_BASE_URL=https://api.groq.com/openai/v1  # Default

# Option B: LM Studio (Local code review with gptoss120 or gpt20)
# - Local models: gptoss120 or gpt20
# - Pros: Free, private
# - Cons: Requires local GPU/CPU
# CODE_REVIEW_PROVIDER=lmstudio
# LMSTUDIO_REVIEW_URL=http://localhost:1234/v1
# LMSTUDIO_REVIEW_MODEL=gptoss120  # or gpt20
# Available models for code review:
#   - gptoss120: Optimized for code review
#   - gpt20: Alternative code review model

# ============================================================================
# EXAMPLE CONFIGURATIONS
# ============================================================================

# Configuration 1: Anthropic Subscription + OpenAI Embeddings + Groq Review
# CLAUDE_CODE_PROVIDER=anthropic-subscription
# ANTHROPIC_SUBSCRIPTION=true
# EMBEDDING_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-your-key-here
# CODE_REVIEW_PROVIDER=groq
# GROQ_API_KEY=gsk_your-key-here

# Configuration 2: Zai Subscription + LM Studio Embeddings + LM Studio Review
# CLAUDE_CODE_PROVIDER=zai-subscription
# ZAI_SUBSCRIPTION=true
# ZAI_ORG_ID=your-org-id-here
# EMBEDDING_PROVIDER=lmstudio
# LMSTUDIO_EMBEDDING_URL=http://localhost:1234/v1
# LMSTUDIO_EMBEDDING_MODEL=nomic-embed-text-v1.5
# CODE_REVIEW_PROVIDER=lmstudio
# LMSTUDIO_REVIEW_URL=http://localhost:1234/v1
# LMSTUDIO_REVIEW_MODEL=gptoss120

# Configuration 3: Anthropic API + OpenAI Embeddings + Groq Review
# CLAUDE_CODE_PROVIDER=anthropic-api
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# EMBEDDING_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-your-key-here
# CODE_REVIEW_PROVIDER=groq
# GROQ_API_KEY=gsk_your-key-here

# ============================================================================
# LM STUDIO SETUP (If using LM Studio for embeddings or code review)
# ============================================================================
# 1. Download LM Studio from https://lmstudio.ai/
# 2. Download models:
#    - For embeddings: nomic-embed-text-v1.5 (our standard)
#    - For code review: gptoss120 or gpt20
# 3. Start the local server in LM Studio (usually port 1234)
# 4. Load the appropriate model
# 5. Configure the URLs and model names above

# ============================================================================
# DATABASE AND VECTOR STORE
# ============================================================================

# Qdrant Configuration (Vector database for embeddings)
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION_PREFIX=hephaestus

# SQLite Database (Task and workflow storage)
DATABASE_PATH=./hephaestus.db

# ============================================================================
# MCP SERVER CONFIGURATION
# ============================================================================

MCP_PORT=8000
MCP_HOST=0.0.0.0

# ============================================================================
# MONITORING AND PERFORMANCE
# ============================================================================

MONITORING_INTERVAL_SECONDS=60
MAX_HEALTH_CHECK_FAILURES=3
AGENT_TIMEOUT_MINUTES=30
MAX_CONCURRENT_AGENTS=10

# Default CLI Tool
DEFAULT_CLI_TOOL=claude  # or "codex"

# ============================================================================
# ADVANCED CONFIGURATION
# ============================================================================

# Custom endpoints (if using proxies or custom deployments)
# ANTHROPIC_BASE_URL=https://your-proxy.com
# GROQ_BASE_URL=https://your-proxy.com/v1
# LMSTUDIO_EMBEDDING_URL=http://192.168.1.100:1234/v1  # Remote LM Studio
# LMSTUDIO_REVIEW_URL=http://192.168.1.100:1234/v1

# Timeout settings (in seconds)
# LLM_TIMEOUT=120
# EMBEDDING_TIMEOUT=60

# Retry settings
# LLM_MAX_RETRIES=3
# LLM_RETRY_DELAY=1

# Context window sizes (adjust based on your model)
# ANTHROPIC_MAX_TOKENS=100000
# GROQ_MAX_TOKENS=32768
# LMSTUDIO_MAX_TOKENS=4096
